---
title: 'Chapter 14: Machine Learning'
output: html_document
---

```{r setup, include=FALSE}
# Thanks to Yao Yu for some excellent work on this script.

knitr::opts_chunk$set(echo = TRUE)
library(infer)
library(tidyverse)
library(tidymodels)
library(rpart.plot)

cces <- read_rds("cces_initial.rds")
```

**Intro:** [Machine Learning](https://davidkane9.github.io/PPBDS/14-machine-learning.html). In today's class, we'll be applying your knowledge of regression models to the magical concept of machine learning. After this lesson, you'll have an introductory understanding of training, testing, validation, and classification.

**Data:** The dataset you'll be working with today is the 2006 -- 2018 Cooperative Congressional Election Study data. This is the same dataset as used in the textbook, but we'll be using slightly different variables. This dataset has already been cleaned to remove non-responses and labeled so it's easier to read (eg: for `gender` 1 is now Male and 2 is Female). `income` has been changed from a factor to numeric where 1 = "Less than 10k" and 12 = "150k+". The variable we'll be trying to predict is how people voted in 2016. From the responses, which we filtered down to just people who voted for Donald Trump or Hilary Clinton, we'll be trying to predict who voted for Trump. In the variable, called "trump", a 1 is someone who voted for Trump and a 0 is someone Clinton.

# Day 1

# Scene 1

**Prompt:** Take a peek at the data and try and figure out what the variables mean. You can find the codebook here: https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/II2DB6/EARRB5&version=4.1 

1) What is the age of oldest person who responded to this survey?

2) What proportion of females voted for Trump? 

```{r}

cces %>%
  arrange(desc(age))

cces %>%
  filter(gender == "Female") %>%
  count(trump, gender) %>%
  mutate(prop = prop.table(n)) %>%
  head(1) %>%
  pull()

```


# Scene 2

**Prompt:** Start by creating a logistic model using every variable to try and predict the Trump vote. Consider using `logistic_reg()`, `set_engine()`, and `fit()`. Save the model to an object named `logistic_mod` and the fit to `logistic_fit`. Feel free to refer to the code from last week or to the code in the *Primer*.

1) Print out the model with the confidence interval and interpret the coefficient of `gender`? (Hint: use the divide-by-four rule.) Explain the association of Trump-voting with variables like `income` and `party` to your smart-but-not-mathematical boss. Why doesn't income matter?

2) What is the expected vote for Dom - who is Male, 37, holds a 4-year degree (placing him in the 5 `educ` bucket), Asian, makes $77,000, with `ideology` and `party` equal to 3. (Hint: use predict().)

```{r}

logistic_mod <- logistic_reg() %>%
  set_engine("glm")
  

logistic_fit <- fit(logistic_mod,
                    data = cces,
                    trump ~ gender + age + educ + race + income + party + ideology)

logistic_fit %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)

```

```{r}
dom <- tibble(gender = "Male",
              age = 37,
              educ = 5,
              race = "Asian",
              income = 8,
              party = 3,
              ideology = 3)

predict(logistic_fit, new_data = dom)

```

# Scene 3

**Prompt:** Great! Now that you have created a logistic model to predict Trump vote, create two more models: CART (rpart) and randomforest. Set the seed to 1005. For rand_forest, set mtry = 7 to use all our predictors and set trees = 50 so your computer doesn't take forever to run.

1) Interpret the output of the CART model (Hint: set extra = "auto" in `prp()`.) Where would Dom fall in this decision tree? 

2) Looking at the randomForest model, what does the OOB error tell us? Interpret the first row, second column of the confusion matrix, what does that 920 represent? How does 920 relate to the class.error of 0.078?


```{r}

set.seed(1005)

cart_mod <- decision_tree() %>%
  set_engine("rpart", model = TRUE) %>%
  set_mode("classification")

cart_fit <- fit(cart_mod,
                 trump ~
                   gender + age + educ + race + income + party + ideology, data = cces)

cart_fit$fit %>%
  prp(extra = "auto")
```

```{r}
rf_mod <- rand_forest(mtry = 7, tree = 50) %>%
  set_engine("randomForest") %>%
  set_mode("classification")

rf_fit <- fit(rf_mod,
              data = cces,
              trump ~ gender + age + educ + race + income + party + ideology)
  

```



# Scene 4

**Prompt:** Now that we have our three models, let's see how well they stack up against each other. Use `predict()` on our three models with cces as the data. Then, use `bind_cols()` to combine the cces data with our predictions. Finally, use `accuracy()` to get the proportion of votes that the model correctly predicted

1) Compare the accuracy of our three models, use bind_rows() to put the three in a nice tibble. Which model appears to be the best?

2) So the randomforest model appears to be the best model, how can we test that this is true? What should we worry about?

